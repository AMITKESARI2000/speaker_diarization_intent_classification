{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single label classification text using pytorch lightning training of transformers\n",
    "pl boilerplate : https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Quadro RTX 6000'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib inline\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from torchmetrics.functional import accuracy, f1_score, auroc\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TYPE</th>\n",
       "      <th>DOCUMENT_CONTENT</th>\n",
       "      <th>DOCUMENT_CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Great News to All Vape Lovers Vapefanz has res...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Vaprcase 2 Review: Lifegrabber√¢‚Ç¨‚Ñ¢s Soluti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>i bought 2 boxes and both has 0.8 and 1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>pod Fundamentals Explained vanilla - When rega...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Pretty fun time streaming some VALORANT with m...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>√¢≈° √Ø¬∏¬è√∞≈∏¬ê¬¢ TURTLE JUICE √∞≈∏¬ê¬¢√¢≈° ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Dude check out blankz pods. They make a Juul c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4837</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Koko prem vape device here√∞≈∏‚Äò‚Ä∞0562220852...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Hero of the day myblu capsules We tell there a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Hello guys im a newbie just upgraded to the dr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4840 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TYPE                                   DOCUMENT_CONTENT  DOCUMENT_CLASS\n",
       "0     TRAIN  Great News to All Vape Lovers Vapefanz has res...               2\n",
       "1     TRAIN  Vaprcase 2 Review: Lifegrabber√¢‚Ç¨‚Ñ¢s Soluti...               1\n",
       "2     TRAIN          i bought 2 boxes and both has 0.8 and 1.2               1\n",
       "3     TRAIN  pod Fundamentals Explained vanilla - When rega...               1\n",
       "4     TRAIN  Pretty fun time streaming some VALORANT with m...               2\n",
       "...     ...                                                ...             ...\n",
       "4835  TRAIN  √¢≈° √Ø¬∏¬è√∞≈∏¬ê¬¢ TURTLE JUICE √∞≈∏¬ê¬¢√¢≈° ...               1\n",
       "4836  TRAIN  Dude check out blankz pods. They make a Juul c...               1\n",
       "4837  TRAIN  Koko prem vape device here√∞≈∏‚Äò‚Ä∞0562220852...               1\n",
       "4838  TRAIN  Hero of the day myblu capsules We tell there a...               2\n",
       "4839  TRAIN  Hello guys im a newbie just upgraded to the dr...               0\n",
       "\n",
       "[4840 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load train, test dataset\n",
    "train_filename = \"train_dropdups.csv\"\n",
    "val_filename = \"validation_dropdups.csv\"\n",
    "test_filename = \"test_dropdups.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_filename)\n",
    "val_df = pd.read_csv(val_filename)\n",
    "test_df = pd.read_csv(test_filename)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique content:  True\n",
      "Null values:  False\n",
      "Average sentence length:  36.20702479338843\n",
      "Std deviation sentence length:  25.99536799296542\n"
     ]
    }
   ],
   "source": [
    "print('Unique content: ', train_df['DOCUMENT_CONTENT'].nunique() == train_df.shape[0])\n",
    "print('Null values: ', train_df.isnull().values.any())\n",
    "print('Average sentence length: ', train_df['DOCUMENT_CONTENT'].str.split().str.len().mean())\n",
    "print('Std deviation sentence length: ', train_df['DOCUMENT_CONTENT'].str.split().str.len().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a frequency plot for each category\n",
    "\n",
    "category_counts = {}\n",
    "for doc_type in train_df['DOCUMENT_CLASS']:\n",
    "    if doc_type not in category_counts.keys():\n",
    "        category_counts[doc_type] = 1\n",
    "    else:\n",
    "        category_counts[doc_type] += 1\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 4.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, ax = plt.subplots()\n",
    "# train_df['DOCUMENT_CLASS'].value_counts().plot(ax=ax, kind='bar')\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "ax = sns.countplot(x='DOCUMENT_CLASS', data=train_df)\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.title(\"Number of comments per class\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4840,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract out data content column and labels\n",
    "X_train = train_df[\"DOCUMENT_CONTENT\"]\n",
    "X_val = val_df[\"DOCUMENT_CONTENT\"]\n",
    "X_test = test_df[\"DOCUMENT_CONTENT\"]\n",
    "y_train = train_df[\"DOCUMENT_CLASS\"].values\n",
    "y_val = val_df[\"DOCUMENT_CLASS\"].values\n",
    "y_test = test_df[\"DOCUMENT_CLASS\"].values\n",
    "\n",
    "# seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting y into one hot vectors like multiclass\n",
    "\n",
    "# encoded_dict = {‘0’:[1,0,0],’1’:[0,1,0], ‘2’:[0,0,1]}\n",
    "name_classes = ['0','1','2']\n",
    "num_classes = len(category_counts.keys())\n",
    "\n",
    "y_train = np.eye(num_classes)[y_train]\n",
    "y_val = np.eye(num_classes)[y_val]\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample X before cleaning: Vaprcase 2 Review: Lifegrabber√¢‚Ç¨‚Ñ¢s Solution to Protect Pax 2 Vaporizers ...#cannabisnews #cannabis #hemp #CBD\n",
      "Sample X after cleaning: Vaprcase Review Lifegrabber s Solution to Protect Pax Vaporizers cannabisnews cannabis hemp CBD\n"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "def rm_non_ascii(text):\n",
    "    # remeove non-ascii characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    # remove digits and underscores\n",
    "    text = re.sub(r'[0-9_:\\-\\'\\\"\\[\\]?/+=.,;!@#$%^&*()<>|{}~]+',' ', text)\n",
    "    # removes mutiple spaces\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"Sample X before cleaning:\", X_train[1])\n",
    "X_train = X_train.apply(rm_non_ascii)\n",
    "X_val = X_val.apply(rm_non_ascii)\n",
    "X_test = X_test.apply(rm_non_ascii)\n",
    "print(\"Sample X after cleaning:\", X_train[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample X before stemming: Vaprcase Review Lifegrabber s Solution to Protect Pax Vaporizers cannabisnews cannabis hemp CBD\n",
      "Sample X after stemming: Vaprcase Review Lifegrabber s Solution to Protect Pax Vaporizers cannabisnews cannabis hemp CBD\n"
     ]
    }
   ],
   "source": [
    "# stemming to bring base word form. lemmatisation not req\n",
    "\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    for i in range(len(words)):\n",
    "        words[i] = ps.stem(words[i])\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "        \n",
    "print(\"Sample X before stemming:\", X_train[1])    \n",
    "# X_train = X_train.apply(stemming)\n",
    "# X_val = X_val.apply(stemming)\n",
    "# X_test = X_test.apply(stemming)\n",
    "print(\"Sample X after stemming:\", X_train[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(y_test, y_pred, title=\"Classifier\"):\n",
    "    # Calculate confusion matrix\n",
    "\n",
    "    matrix=confusion_matrix(y_test, y_pred)\n",
    "    cm_labeled=pd.DataFrame(matrix,index=['0','1','2'],columns=['0','1','2'])\n",
    "    # True Labels in y-axis/rows. Precision = TP/(TP+FP)\n",
    "#     print(\"Confusion Matrix:\\n\", cm_labeled)\n",
    "    sns.heatmap(cm_labeled, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    # Get the accuracy\n",
    "    print('\\nAccuracy: {:.5f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "    # Print the classification report\n",
    "    print(f'Classification Report {title}:')\n",
    "#     TODO fix target names dynamic\n",
    "    print(classification_report(y_test, y_pred, target_names=['DOC_0', 'DOC_1', 'DOC_2']))\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To utilize pytorch-lightning’s Lightning Module class\n",
    "1) train_dataloader, (X. y_label, attention_mask, token_type_ids)\n",
    "\n",
    "2) training_step and \n",
    "\n",
    "3) configure_optimizers\n",
    "\n",
    "`VapeSLC train.csv dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VapeSLCDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer,\n",
    "#         tokenizer: BertTokenizer, RobertaTokenizer\n",
    "        max_token_len: int = 100,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        content = data_row['DOCUMENT_CONTENT'] # X \"hi bro vape\"\n",
    "        labels = data_row['DOCUMENT_CLASS'] # y [0,1,0] given as label\n",
    "        labels = np.eye(num_classes)[labels]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            content,\n",
    "            max_length=self.max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True, \n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # BertForSequenceClassification needs input_ids, attention_mask, token_type_ids\n",
    "        return dict(\n",
    "            content=content,\n",
    "            input_ids=encoding['input_ids'].flatten(),\n",
    "            attention_mask=encoding['attention_mask'].flatten(),\n",
    "            labels=torch.FloatTensor(labels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap our custom dataset into a LightningDataModule. \n",
    "# encapsulates all data loading logic and returns the necessary data loaders.\n",
    "# DataLoader helps save on memory during training, entire dataset does not need to be loaded into memory\n",
    "class VapeSLCDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        batch_size=8,\n",
    "        max_token_len=128\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.val_df = val_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = VapeSLCDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.test_dataset = VapeSLCDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.val_dataset = VapeSLCDataset(\n",
    "            self.val_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=2\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model pack everything in a LightningModule\n",
    "# points of interest are the way we configure the optimizers and calculating the area under ROC\n",
    "class SingleLabelClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        num_training_steps=None,\n",
    "        num_warmup_steps=None,\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes),\n",
    "        # self.model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes),\n",
    "        criterion = BCEWithLogitsLoss()\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output_model = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = output_model[0]\n",
    "#         loss_func = BCEWithLogitsLoss() \n",
    "    \n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            #convert labels to float for calculation\n",
    "            loss = self.criterion(logits.view(-1,num_classes),labels.type_as(logits).view(-1,num_classes)) \n",
    "\n",
    "#       TODO:check this outputs or logits\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "          for out_labels in output[\"labels\"].detach().cpu():\n",
    "            labels.append(out_labels)\n",
    "          for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "            predictions.append(out_predictions)\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        for i, name in enumerate(name_classes):\n",
    "          class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "          self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # setting custom optimization parameters. may implement a scheduler here as well.\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        # print(\"named parameters: \", param_optimizer)\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "#TODO         optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True, )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.num_warmup_steps,\n",
    "            num_training_steps=self.num_training_steps\n",
    "        )\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x324 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear learning rate scheduling over training steps\n",
    "\n",
    "dummy_model = torch.nn.Linear(2, 1)\n",
    "optimizer = AdamW(params=dummy_model.parameters(), lr=0.001)\n",
    "warmup_steps = 20\n",
    "total_training_steps = 100\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=warmup_steps,\n",
    "  num_training_steps=total_training_steps\n",
    ")\n",
    "learning_rate_history = []\n",
    "for step in range(total_training_steps):\n",
    "  optimizer.step()\n",
    "  scheduler.step()\n",
    "  learning_rate_history.append(optimizer.param_groups[0]['lr'])\n",
    "plt.plot(learning_rate_history, label=\"learning rate\")\n",
    "plt.axvline(x=warmup_steps, color=\"red\", linestyle=(0, (5, 10)), label=\"warmup end\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/microsoft/mpnet-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/9a24bf5e4745b7a1fc2af13b404a440cabd4e3bbf3dfead89248ca3cb56cff11.98b26f9c960899aa0e99c10a12750104e467743b3b460b79fa7d76907549319b\n",
      "loading file https://huggingface.co/microsoft/mpnet-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/mpnet-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/mpnet-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/microsoft/mpnet-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a18491990fc4d73f8f8bc4048f33f3210d3bbfa158c7073345c34a0f69d55812.12e86a96e35480ba3a7aa8123711b61621201f63ce5060bfe0f58b932f7427c5\n",
      "Model config MPNetConfig {\n",
      "  \"_name_or_path\": \"microsoft/mpnet-base\",\n",
      "  \"architectures\": [\n",
      "    \"MPNetForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"mpnet\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30527\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got datatype:  dict_keys(['content', 'input_ids', 'attention_mask', 'labels'])\n",
      "Vaprcase 2 Review: Lifegrabber√¢‚Ç¨‚Ñ¢s Solution to Protect Pax 2 Vaporizers ...#cannabisnews #cannabis #hemp #CBD  \n",
      "tensor([0., 1., 0.]) --->\n",
      "input encoding : tensor([    0, 12440, 18102, 18386,  1020,  3323,  1028,  2170, 17647, 29329,\n",
      "        30131, 29649,  1526,  1043, 29655,  1526,  1054, 29649,  2019,  5580,\n",
      "         2004,  4051,  6647,  2599,  1020, 20068, 17633,  2019,  1016,  1016,\n",
      "         1016,  1005, 17989,  2642,  9337,  1005, 17989,  1005, 19614,  2365,\n",
      "         1005, 17328,  2098,     2,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "attention mask : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]) \n"
     ]
    }
   ],
   "source": [
    "# sample item from the dataset\n",
    "\n",
    "# BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "# # XLNet tokenizer\n",
    "# tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=False) \n",
    "# # RoBERTa tokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=False)\n",
    "# MPNet\n",
    "tokenizer = MPNetTokenizer.from_pretrained(\"microsoft/mpnet-base\")\n",
    "\n",
    "\n",
    "MAX_TOKEN_COUNT = 200\n",
    "train_dataset = VapeSLCDataset(\n",
    "  train_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "sample_item = train_dataset[1]\n",
    "print(\"Got datatype: \",sample_item.keys())\n",
    "print(f\"{sample_item['content']}  \\n{sample_item['labels']} --->\")\n",
    "print(f\"input encoding : {sample_item['input_ids']}\\nattention mask : {sample_item['attention_mask']} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2\n",
    "BATCH_SIZE = 64\n",
    "data_module = VapeSLCDataModule(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    val_df=val_df,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=MAX_TOKEN_COUNT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/mpnet-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a18491990fc4d73f8f8bc4048f33f3210d3bbfa158c7073345c34a0f69d55812.12e86a96e35480ba3a7aa8123711b61621201f63ce5060bfe0f58b932f7427c5\n",
      "Model config MPNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"MPNetForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"mpnet\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30527\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/mpnet-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/44c2dd9f4ad6557c501ae1216dcae588d1e26c13aeb2050ee38bd12ebb37dbfd.230c66a7c9cce5727f86291a6caa86b4b99d0ef821e1bbfbad10ef1875f60d59\n",
      "Some weights of the model checkpoint at microsoft/mpnet-base were not used when initializing MPNetForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SingleLabelClassifier(\n",
       "  (model): MPNetForSequenceClassification(\n",
       "    (mpnet): MPNetModel(\n",
       "      (embeddings): MPNetEmbeddings(\n",
       "        (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): MPNetEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): MPNetLayer(\n",
       "            (attention): MPNetAttention(\n",
       "              (attn): MPNetSelfAttention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate): MPNetIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): MPNetOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (relative_attention_bias): Embedding(32, 12)\n",
       "      )\n",
       "    )\n",
       "    (classifier): MPNetClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of training steps per epoch = number of training examples / batch size\n",
    "# total training steps = training steps per epoch * number of epochs\n",
    "\n",
    "steps_per_epoch=len(train_df) # BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps\n",
    "\n",
    "\n",
    "# BERT\n",
    "# model_name = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n",
    "# XLNet:\n",
    "# model_name = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=num_classes)\n",
    "# RoBERTa:\n",
    "# model_name = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes)\n",
    "# MPNet \n",
    "model_name = MPNetForSequenceClassification.from_pretrained(\"microsoft/mpnet-base\", num_labels=num_classes)\n",
    "\n",
    "\n",
    "# Model Init\n",
    "model = SingleLabelClassifier(\n",
    "    num_classes=num_classes,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_training_steps,\n",
    "    model=model_name\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "##### Training with PyTorch Lightning, we can build a standard pipeline and train (almost?) every model.\n",
    "\n",
    "Use at least 3 components:\n",
    "- Checkpointing that saves the best model (based on validation loss):\n",
    "- Log the progress in TensorBoard\n",
    "- Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f9ae6d72520>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f9ae6d72520>)`.\n",
      "  rank_zero_deprecation(\n",
      "/root/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output_dir = 'checkpoints'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "dirpath = os.path.join(os.getcwd(), output_dir)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=dirpath,\n",
    "  filename=\"bert-checkpoint\",\n",
    "  save_top_k=1,\n",
    "  verbose=True,\n",
    "  monitor=\"val_loss\",\n",
    "  mode=\"min\"\n",
    ")\n",
    "\n",
    "# logger\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"single-label-classifier\")\n",
    "\n",
    "# early stopping triggers when the loss hasn’t improved for the last 2 epochs\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "\n",
    "# train!\n",
    "trainer = pl.Trainer(\n",
    "  logger=logger,\n",
    "  checkpoint_callback=checkpoint_callback,\n",
    "  callbacks=[early_stopping_callback],\n",
    "  max_epochs=N_EPOCHS,\n",
    "  gpus=1,\n",
    "  auto_lr_find=True,\n",
    "  progress_bar_refresh_rate=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type                           | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model     | MPNetForSequenceClassification | 109 M \n",
      "1 | criterion | BCEWithLogitsLoss              | 0     \n",
      "-------------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.955   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/root/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7fc1a3c53a49fd8639b3c62283840e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SingleLabelClassifier:\n\tMissing key(s) in state_dict: \"model.bert.embeddings.position_ids\", \"model.bert.embeddings.word_embeddings.weight\", \"model.bert.embeddings.position_embeddings.weight\", \"model.bert.embeddings.token_type_embeddings.weight\", \"model.bert.embeddings.LayerNorm.weight\", \"model.bert.embeddings.LayerNorm.bias\", \"model.bert.encoder.layer.0.attention.self.query.weight\", \"model.bert.encoder.layer.0.attention.self.query.bias\", \"model.bert.encoder.layer.0.attention.self.key.weight\", \"model.bert.encoder.layer.0.attention.self.key.bias\", \"model.bert.encoder.layer.0.attention.self.value.weight\", \"model.bert.encoder.layer.0.attention.self.value.bias\", \"model.bert.encoder.layer.0.attention.output.dense.weight\", \"model.bert.encoder.layer.0.attention.output.dense.bias\", \"model.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.0.intermediate.dense.weight\", \"model.bert.encoder.layer.0.intermediate.dense.bias\", \"model.bert.encoder.layer.0.output.dense.weight\", \"model.bert.encoder.layer.0.output.dense.bias\", \"model.bert.encoder.layer.0.output.LayerNorm.weight\", \"model.bert.encoder.layer.0.output.LayerNorm.bias\", \"model.bert.encoder.layer.1.attention.self.query.weight\", \"model.bert.encoder.layer.1.attention.self.query.bias\", \"model.bert.encoder.layer.1.attention.self.key.weight\", \"model.bert.encoder.layer.1.attention.self.key.bias\", \"model.bert.encoder.layer.1.attention.self.value.weight\", \"model.bert.encoder.layer.1.attention.self.value.bias\", \"model.bert.encoder.layer.1.attention.output.dense.weight\", \"model.bert.encoder.layer.1.attention.output.dense.bias\", \"model.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.1.intermediate.dense.weight\", \"model.bert.encoder.layer.1.intermediate.dense.bias\", \"model.bert.encoder.layer.1.output.dense.weight\", \"model.bert.encoder.layer.1.output.dense.bias\", \"model.bert.encoder.layer.1.output.LayerNorm.weight\", \"model.bert.encoder.layer.1.output.LayerNorm.bias\", \"model.bert.encoder.layer.2.attention.self.query.weight\", \"model.bert.encoder.layer.2.attention.self.query.bias\", \"model.bert.encoder.layer.2.attention.self.key.weight\", \"model.bert.encoder.layer.2.attention.self.key.bias\", \"model.bert.encoder.layer.2.attention.self.value.weight\", \"model.bert.encoder.layer.2.attention.self.value.bias\", \"model.bert.encoder.layer.2.attention.output.dense.weight\", \"model.bert.encoder.layer.2.attention.output.dense.bias\", \"model.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.2.intermediate.dense.weight\", \"model.bert.encoder.layer.2.intermediate.dense.bias\", \"model.bert.encoder.layer.2.output.dense.weight\", \"model.bert.encoder.layer.2.output.dense.bias\", \"model.bert.encoder.layer.2.output.LayerNorm.weight\", \"model.bert.encoder.layer.2.output.LayerNorm.bias\", \"model.bert.encoder.layer.3.attention.self.query.weight\", \"model.bert.encoder.layer.3.attention.self.query.bias\", \"model.bert.encoder.layer.3.attention.self.key.weight\", \"model.bert.encoder.layer.3.attention.self.key.bias\", \"model.bert.encoder.layer.3.attention.self.value.weight\", \"model.bert.encoder.layer.3.attention.self.value.bias\", \"model.bert.encoder.layer.3.attention.output.dense.weight\", \"model.bert.encoder.layer.3.attention.output.dense.bias\", \"model.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.3.intermediate.dense.weight\", \"model.bert.encoder.layer.3.intermediate.dense.bias\", \"model.bert.encoder.layer.3.output.dense.weight\", \"model.bert.encoder.layer.3.output.dense.bias\", \"model.bert.encoder.layer.3.output.LayerNorm.weight\", \"model.bert.encoder.layer.3.output.LayerNorm.bias\", \"model.bert.encoder.layer.4.attention.self.query.weight\", \"model.bert.encoder.layer.4.attention.self.query.bias\", \"model.bert.encoder.layer.4.attention.self.key.weight\", \"model.bert.encoder.layer.4.attention.self.key.bias\", \"model.bert.encoder.layer.4.attention.self.value.weight\", \"model.bert.encoder.layer.4.attention.self.value.bias\", \"model.bert.encoder.layer.4.attention.output.dense.weight\", \"model.bert.encoder.layer.4.attention.output.dense.bias\", \"model.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.4.intermediate.dense.weight\", \"model.bert.encoder.layer.4.intermediate.dense.bias\", \"model.bert.encoder.layer.4.output.dense.weight\", \"model.bert.encoder.layer.4.output.dense.bias\", \"model.bert.encoder.layer.4.output.LayerNorm.weight\", \"model.bert.encoder.layer.4.output.LayerNorm.bias\", \"model.bert.encoder.layer.5.attention.self.query.weight\", \"model.bert.encoder.layer.5.attention.self.query.bias\", \"model.bert.encoder.layer.5.attention.self.key.weight\", \"model.bert.encoder.layer.5.attention.self.key.bias\", \"model.bert.encoder.layer.5.attention.self.value.weight\", \"model.bert.encoder.layer.5.attention.self.value.bias\", \"model.bert.encoder.layer.5.attention.output.dense.weight\", \"model.bert.encoder.layer.5.attention.output.dense.bias\", \"model.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.5.intermediate.dense.weight\", \"model.bert.encoder.layer.5.intermediate.dense.bias\", \"model.bert.encoder.layer.5.output.dense.weight\", \"model.bert.encoder.layer.5.output.dense.bias\", \"model.bert.encoder.layer.5.output.LayerNorm.weight\", \"model.bert.encoder.layer.5.output.LayerNorm.bias\", \"model.bert.encoder.layer.6.attention.self.query.weight\", \"model.bert.encoder.layer.6.attention.self.query.bias\", \"model.bert.encoder.layer.6.attention.self.key.weight\", \"model.bert.encoder.layer.6.attention.self.key.bias\", \"model.bert.encoder.layer.6.attention.self.value.weight\", \"model.bert.encoder.layer.6.attention.self.value.bias\", \"model.bert.encoder.layer.6.attention.output.dense.weight\", \"model.bert.encoder.layer.6.attention.output.dense.bias\", \"model.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.6.intermediate.dense.weight\", \"model.bert.encoder.layer.6.intermediate.dense.bias\", \"model.bert.encoder.layer.6.output.dense.weight\", \"model.bert.encoder.layer.6.output.dense.bias\", \"model.bert.encoder.layer.6.output.LayerNorm.weight\", \"model.bert.encoder.layer.6.output.LayerNorm.bias\", \"model.bert.encoder.layer.7.attention.self.query.weight\", \"model.bert.encoder.layer.7.attention.self.query.bias\", \"model.bert.encoder.layer.7.attention.self.key.weight\", \"model.bert.encoder.layer.7.attention.self.key.bias\", \"model.bert.encoder.layer.7.attention.self.value.weight\", \"model.bert.encoder.layer.7.attention.self.value.bias\", \"model.bert.encoder.layer.7.attention.output.dense.weight\", \"model.bert.encoder.layer.7.attention.output.dense.bias\", \"model.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.7.intermediate.dense.weight\", \"model.bert.encoder.layer.7.intermediate.dense.bias\", \"model.bert.encoder.layer.7.output.dense.weight\", \"model.bert.encoder.layer.7.output.dense.bias\", \"model.bert.encoder.layer.7.output.LayerNorm.weight\", \"model.bert.encoder.layer.7.output.LayerNorm.bias\", \"model.bert.encoder.layer.8.attention.self.query.weight\", \"model.bert.encoder.layer.8.attention.self.query.bias\", \"model.bert.encoder.layer.8.attention.self.key.weight\", \"model.bert.encoder.layer.8.attention.self.key.bias\", \"model.bert.encoder.layer.8.attention.self.value.weight\", \"model.bert.encoder.layer.8.attention.self.value.bias\", \"model.bert.encoder.layer.8.attention.output.dense.weight\", \"model.bert.encoder.layer.8.attention.output.dense.bias\", \"model.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.8.intermediate.dense.weight\", \"model.bert.encoder.layer.8.intermediate.dense.bias\", \"model.bert.encoder.layer.8.output.dense.weight\", \"model.bert.encoder.layer.8.output.dense.bias\", \"model.bert.encoder.layer.8.output.LayerNorm.weight\", \"model.bert.encoder.layer.8.output.LayerNorm.bias\", \"model.bert.encoder.layer.9.attention.self.query.weight\", \"model.bert.encoder.layer.9.attention.self.query.bias\", \"model.bert.encoder.layer.9.attention.self.key.weight\", \"model.bert.encoder.layer.9.attention.self.key.bias\", \"model.bert.encoder.layer.9.attention.self.value.weight\", \"model.bert.encoder.layer.9.attention.self.value.bias\", \"model.bert.encoder.layer.9.attention.output.dense.weight\", \"model.bert.encoder.layer.9.attention.output.dense.bias\", \"model.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.9.intermediate.dense.weight\", \"model.bert.encoder.layer.9.intermediate.dense.bias\", \"model.bert.encoder.layer.9.output.dense.weight\", \"model.bert.encoder.layer.9.output.dense.bias\", \"model.bert.encoder.layer.9.output.LayerNorm.weight\", \"model.bert.encoder.layer.9.output.LayerNorm.bias\", \"model.bert.encoder.layer.10.attention.self.query.weight\", \"model.bert.encoder.layer.10.attention.self.query.bias\", \"model.bert.encoder.layer.10.attention.self.key.weight\", \"model.bert.encoder.layer.10.attention.self.key.bias\", \"model.bert.encoder.layer.10.attention.self.value.weight\", \"model.bert.encoder.layer.10.attention.self.value.bias\", \"model.bert.encoder.layer.10.attention.output.dense.weight\", \"model.bert.encoder.layer.10.attention.output.dense.bias\", \"model.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.10.intermediate.dense.weight\", \"model.bert.encoder.layer.10.intermediate.dense.bias\", \"model.bert.encoder.layer.10.output.dense.weight\", \"model.bert.encoder.layer.10.output.dense.bias\", \"model.bert.encoder.layer.10.output.LayerNorm.weight\", \"model.bert.encoder.layer.10.output.LayerNorm.bias\", \"model.bert.encoder.layer.11.attention.self.query.weight\", \"model.bert.encoder.layer.11.attention.self.query.bias\", \"model.bert.encoder.layer.11.attention.self.key.weight\", \"model.bert.encoder.layer.11.attention.self.key.bias\", \"model.bert.encoder.layer.11.attention.self.value.weight\", \"model.bert.encoder.layer.11.attention.self.value.bias\", \"model.bert.encoder.layer.11.attention.output.dense.weight\", \"model.bert.encoder.layer.11.attention.output.dense.bias\", \"model.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.11.intermediate.dense.weight\", \"model.bert.encoder.layer.11.intermediate.dense.bias\", \"model.bert.encoder.layer.11.output.dense.weight\", \"model.bert.encoder.layer.11.output.dense.bias\", \"model.bert.encoder.layer.11.output.LayerNorm.weight\", \"model.bert.encoder.layer.11.output.LayerNorm.bias\", \"model.bert.pooler.dense.weight\", \"model.bert.pooler.dense.bias\", \"model.classifier.weight\", \"model.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"model.mpnet.embeddings.position_ids\", \"model.mpnet.embeddings.word_embeddings.weight\", \"model.mpnet.embeddings.position_embeddings.weight\", \"model.mpnet.embeddings.LayerNorm.weight\", \"model.mpnet.embeddings.LayerNorm.bias\", \"model.mpnet.encoder.layer.0.attention.attn.q.weight\", \"model.mpnet.encoder.layer.0.attention.attn.q.bias\", \"model.mpnet.encoder.layer.0.attention.attn.k.weight\", \"model.mpnet.encoder.layer.0.attention.attn.k.bias\", \"model.mpnet.encoder.layer.0.attention.attn.v.weight\", \"model.mpnet.encoder.layer.0.attention.attn.v.bias\", \"model.mpnet.encoder.layer.0.attention.attn.o.weight\", \"model.mpnet.encoder.layer.0.attention.attn.o.bias\", \"model.mpnet.encoder.layer.0.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.0.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.0.intermediate.dense.weight\", \"model.mpnet.encoder.layer.0.intermediate.dense.bias\", \"model.mpnet.encoder.layer.0.output.dense.weight\", \"model.mpnet.encoder.layer.0.output.dense.bias\", \"model.mpnet.encoder.layer.0.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.0.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.1.attention.attn.q.weight\", \"model.mpnet.encoder.layer.1.attention.attn.q.bias\", \"model.mpnet.encoder.layer.1.attention.attn.k.weight\", \"model.mpnet.encoder.layer.1.attention.attn.k.bias\", \"model.mpnet.encoder.layer.1.attention.attn.v.weight\", \"model.mpnet.encoder.layer.1.attention.attn.v.bias\", \"model.mpnet.encoder.layer.1.attention.attn.o.weight\", \"model.mpnet.encoder.layer.1.attention.attn.o.bias\", \"model.mpnet.encoder.layer.1.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.1.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.1.intermediate.dense.weight\", \"model.mpnet.encoder.layer.1.intermediate.dense.bias\", \"model.mpnet.encoder.layer.1.output.dense.weight\", \"model.mpnet.encoder.layer.1.output.dense.bias\", \"model.mpnet.encoder.layer.1.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.1.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.2.attention.attn.q.weight\", \"model.mpnet.encoder.layer.2.attention.attn.q.bias\", \"model.mpnet.encoder.layer.2.attention.attn.k.weight\", \"model.mpnet.encoder.layer.2.attention.attn.k.bias\", \"model.mpnet.encoder.layer.2.attention.attn.v.weight\", \"model.mpnet.encoder.layer.2.attention.attn.v.bias\", \"model.mpnet.encoder.layer.2.attention.attn.o.weight\", \"model.mpnet.encoder.layer.2.attention.attn.o.bias\", \"model.mpnet.encoder.layer.2.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.2.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.2.intermediate.dense.weight\", \"model.mpnet.encoder.layer.2.intermediate.dense.bias\", \"model.mpnet.encoder.layer.2.output.dense.weight\", \"model.mpnet.encoder.layer.2.output.dense.bias\", \"model.mpnet.encoder.layer.2.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.2.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.3.attention.attn.q.weight\", \"model.mpnet.encoder.layer.3.attention.attn.q.bias\", \"model.mpnet.encoder.layer.3.attention.attn.k.weight\", \"model.mpnet.encoder.layer.3.attention.attn.k.bias\", \"model.mpnet.encoder.layer.3.attention.attn.v.weight\", \"model.mpnet.encoder.layer.3.attention.attn.v.bias\", \"model.mpnet.encoder.layer.3.attention.attn.o.weight\", \"model.mpnet.encoder.layer.3.attention.attn.o.bias\", \"model.mpnet.encoder.layer.3.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.3.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.3.intermediate.dense.weight\", \"model.mpnet.encoder.layer.3.intermediate.dense.bias\", \"model.mpnet.encoder.layer.3.output.dense.weight\", \"model.mpnet.encoder.layer.3.output.dense.bias\", \"model.mpnet.encoder.layer.3.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.3.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.4.attention.attn.q.weight\", \"model.mpnet.encoder.layer.4.attention.attn.q.bias\", \"model.mpnet.encoder.layer.4.attention.attn.k.weight\", \"model.mpnet.encoder.layer.4.attention.attn.k.bias\", \"model.mpnet.encoder.layer.4.attention.attn.v.weight\", \"model.mpnet.encoder.layer.4.attention.attn.v.bias\", \"model.mpnet.encoder.layer.4.attention.attn.o.weight\", \"model.mpnet.encoder.layer.4.attention.attn.o.bias\", \"model.mpnet.encoder.layer.4.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.4.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.4.intermediate.dense.weight\", \"model.mpnet.encoder.layer.4.intermediate.dense.bias\", \"model.mpnet.encoder.layer.4.output.dense.weight\", \"model.mpnet.encoder.layer.4.output.dense.bias\", \"model.mpnet.encoder.layer.4.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.4.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.5.attention.attn.q.weight\", \"model.mpnet.encoder.layer.5.attention.attn.q.bias\", \"model.mpnet.encoder.layer.5.attention.attn.k.weight\", \"model.mpnet.encoder.layer.5.attention.attn.k.bias\", \"model.mpnet.encoder.layer.5.attention.attn.v.weight\", \"model.mpnet.encoder.layer.5.attention.attn.v.bias\", \"model.mpnet.encoder.layer.5.attention.attn.o.weight\", \"model.mpnet.encoder.layer.5.attention.attn.o.bias\", \"model.mpnet.encoder.layer.5.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.5.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.5.intermediate.dense.weight\", \"model.mpnet.encoder.layer.5.intermediate.dense.bias\", \"model.mpnet.encoder.layer.5.output.dense.weight\", \"model.mpnet.encoder.layer.5.output.dense.bias\", \"model.mpnet.encoder.layer.5.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.5.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.6.attention.attn.q.weight\", \"model.mpnet.encoder.layer.6.attention.attn.q.bias\", \"model.mpnet.encoder.layer.6.attention.attn.k.weight\", \"model.mpnet.encoder.layer.6.attention.attn.k.bias\", \"model.mpnet.encoder.layer.6.attention.attn.v.weight\", \"model.mpnet.encoder.layer.6.attention.attn.v.bias\", \"model.mpnet.encoder.layer.6.attention.attn.o.weight\", \"model.mpnet.encoder.layer.6.attention.attn.o.bias\", \"model.mpnet.encoder.layer.6.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.6.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.6.intermediate.dense.weight\", \"model.mpnet.encoder.layer.6.intermediate.dense.bias\", \"model.mpnet.encoder.layer.6.output.dense.weight\", \"model.mpnet.encoder.layer.6.output.dense.bias\", \"model.mpnet.encoder.layer.6.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.6.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.7.attention.attn.q.weight\", \"model.mpnet.encoder.layer.7.attention.attn.q.bias\", \"model.mpnet.encoder.layer.7.attention.attn.k.weight\", \"model.mpnet.encoder.layer.7.attention.attn.k.bias\", \"model.mpnet.encoder.layer.7.attention.attn.v.weight\", \"model.mpnet.encoder.layer.7.attention.attn.v.bias\", \"model.mpnet.encoder.layer.7.attention.attn.o.weight\", \"model.mpnet.encoder.layer.7.attention.attn.o.bias\", \"model.mpnet.encoder.layer.7.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.7.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.7.intermediate.dense.weight\", \"model.mpnet.encoder.layer.7.intermediate.dense.bias\", \"model.mpnet.encoder.layer.7.output.dense.weight\", \"model.mpnet.encoder.layer.7.output.dense.bias\", \"model.mpnet.encoder.layer.7.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.7.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.8.attention.attn.q.weight\", \"model.mpnet.encoder.layer.8.attention.attn.q.bias\", \"model.mpnet.encoder.layer.8.attention.attn.k.weight\", \"model.mpnet.encoder.layer.8.attention.attn.k.bias\", \"model.mpnet.encoder.layer.8.attention.attn.v.weight\", \"model.mpnet.encoder.layer.8.attention.attn.v.bias\", \"model.mpnet.encoder.layer.8.attention.attn.o.weight\", \"model.mpnet.encoder.layer.8.attention.attn.o.bias\", \"model.mpnet.encoder.layer.8.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.8.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.8.intermediate.dense.weight\", \"model.mpnet.encoder.layer.8.intermediate.dense.bias\", \"model.mpnet.encoder.layer.8.output.dense.weight\", \"model.mpnet.encoder.layer.8.output.dense.bias\", \"model.mpnet.encoder.layer.8.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.8.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.9.attention.attn.q.weight\", \"model.mpnet.encoder.layer.9.attention.attn.q.bias\", \"model.mpnet.encoder.layer.9.attention.attn.k.weight\", \"model.mpnet.encoder.layer.9.attention.attn.k.bias\", \"model.mpnet.encoder.layer.9.attention.attn.v.weight\", \"model.mpnet.encoder.layer.9.attention.attn.v.bias\", \"model.mpnet.encoder.layer.9.attention.attn.o.weight\", \"model.mpnet.encoder.layer.9.attention.attn.o.bias\", \"model.mpnet.encoder.layer.9.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.9.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.9.intermediate.dense.weight\", \"model.mpnet.encoder.layer.9.intermediate.dense.bias\", \"model.mpnet.encoder.layer.9.output.dense.weight\", \"model.mpnet.encoder.layer.9.output.dense.bias\", \"model.mpnet.encoder.layer.9.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.9.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.10.attention.attn.q.weight\", \"model.mpnet.encoder.layer.10.attention.attn.q.bias\", \"model.mpnet.encoder.layer.10.attention.attn.k.weight\", \"model.mpnet.encoder.layer.10.attention.attn.k.bias\", \"model.mpnet.encoder.layer.10.attention.attn.v.weight\", \"model.mpnet.encoder.layer.10.attention.attn.v.bias\", \"model.mpnet.encoder.layer.10.attention.attn.o.weight\", \"model.mpnet.encoder.layer.10.attention.attn.o.bias\", \"model.mpnet.encoder.layer.10.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.10.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.10.intermediate.dense.weight\", \"model.mpnet.encoder.layer.10.intermediate.dense.bias\", \"model.mpnet.encoder.layer.10.output.dense.weight\", \"model.mpnet.encoder.layer.10.output.dense.bias\", \"model.mpnet.encoder.layer.10.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.10.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.11.attention.attn.q.weight\", \"model.mpnet.encoder.layer.11.attention.attn.q.bias\", \"model.mpnet.encoder.layer.11.attention.attn.k.weight\", \"model.mpnet.encoder.layer.11.attention.attn.k.bias\", \"model.mpnet.encoder.layer.11.attention.attn.v.weight\", \"model.mpnet.encoder.layer.11.attention.attn.v.bias\", \"model.mpnet.encoder.layer.11.attention.attn.o.weight\", \"model.mpnet.encoder.layer.11.attention.attn.o.bias\", \"model.mpnet.encoder.layer.11.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.11.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.11.intermediate.dense.weight\", \"model.mpnet.encoder.layer.11.intermediate.dense.bias\", \"model.mpnet.encoder.layer.11.output.dense.weight\", \"model.mpnet.encoder.layer.11.output.dense.bias\", \"model.mpnet.encoder.layer.11.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.11.output.LayerNorm.bias\", \"model.mpnet.encoder.relative_attention_bias.weight\", \"model.classifier.dense.weight\", \"model.classifier.dense.bias\", \"model.classifier.out_proj.weight\", \"model.classifier.out_proj.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the best val_loss model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mSingleLabelClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m trained_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m trained_model\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:161\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# override the hparams with values that were passed in\u001b[39;00m\n\u001b[1;32m    159\u001b[0m checkpoint[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mCHECKPOINT_HYPER_PARAMS_KEY]\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 161\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:209\u001b[0m, in \u001b[0;36mModelIO._load_model_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    206\u001b[0m model\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SingleLabelClassifier:\n\tMissing key(s) in state_dict: \"model.bert.embeddings.position_ids\", \"model.bert.embeddings.word_embeddings.weight\", \"model.bert.embeddings.position_embeddings.weight\", \"model.bert.embeddings.token_type_embeddings.weight\", \"model.bert.embeddings.LayerNorm.weight\", \"model.bert.embeddings.LayerNorm.bias\", \"model.bert.encoder.layer.0.attention.self.query.weight\", \"model.bert.encoder.layer.0.attention.self.query.bias\", \"model.bert.encoder.layer.0.attention.self.key.weight\", \"model.bert.encoder.layer.0.attention.self.key.bias\", \"model.bert.encoder.layer.0.attention.self.value.weight\", \"model.bert.encoder.layer.0.attention.self.value.bias\", \"model.bert.encoder.layer.0.attention.output.dense.weight\", \"model.bert.encoder.layer.0.attention.output.dense.bias\", \"model.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.0.intermediate.dense.weight\", \"model.bert.encoder.layer.0.intermediate.dense.bias\", \"model.bert.encoder.layer.0.output.dense.weight\", \"model.bert.encoder.layer.0.output.dense.bias\", \"model.bert.encoder.layer.0.output.LayerNorm.weight\", \"model.bert.encoder.layer.0.output.LayerNorm.bias\", \"model.bert.encoder.layer.1.attention.self.query.weight\", \"model.bert.encoder.layer.1.attention.self.query.bias\", \"model.bert.encoder.layer.1.attention.self.key.weight\", \"model.bert.encoder.layer.1.attention.self.key.bias\", \"model.bert.encoder.layer.1.attention.self.value.weight\", \"model.bert.encoder.layer.1.attention.self.value.bias\", \"model.bert.encoder.layer.1.attention.output.dense.weight\", \"model.bert.encoder.layer.1.attention.output.dense.bias\", \"model.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.1.intermediate.dense.weight\", \"model.bert.encoder.layer.1.intermediate.dense.bias\", \"model.bert.encoder.layer.1.output.dense.weight\", \"model.bert.encoder.layer.1.output.dense.bias\", \"model.bert.encoder.layer.1.output.LayerNorm.weight\", \"model.bert.encoder.layer.1.output.LayerNorm.bias\", \"model.bert.encoder.layer.2.attention.self.query.weight\", \"model.bert.encoder.layer.2.attention.self.query.bias\", \"model.bert.encoder.layer.2.attention.self.key.weight\", \"model.bert.encoder.layer.2.attention.self.key.bias\", \"model.bert.encoder.layer.2.attention.self.value.weight\", \"model.bert.encoder.layer.2.attention.self.value.bias\", \"model.bert.encoder.layer.2.attention.output.dense.weight\", \"model.bert.encoder.layer.2.attention.output.dense.bias\", \"model.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.2.intermediate.dense.weight\", \"model.bert.encoder.layer.2.intermediate.dense.bias\", \"model.bert.encoder.layer.2.output.dense.weight\", \"model.bert.encoder.layer.2.output.dense.bias\", \"model.bert.encoder.layer.2.output.LayerNorm.weight\", \"model.bert.encoder.layer.2.output.LayerNorm.bias\", \"model.bert.encoder.layer.3.attention.self.query.weight\", \"model.bert.encoder.layer.3.attention.self.query.bias\", \"model.bert.encoder.layer.3.attention.self.key.weight\", \"model.bert.encoder.layer.3.attention.self.key.bias\", \"model.bert.encoder.layer.3.attention.self.value.weight\", \"model.bert.encoder.layer.3.attention.self.value.bias\", \"model.bert.encoder.layer.3.attention.output.dense.weight\", \"model.bert.encoder.layer.3.attention.output.dense.bias\", \"model.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.3.intermediate.dense.weight\", \"model.bert.encoder.layer.3.intermediate.dense.bias\", \"model.bert.encoder.layer.3.output.dense.weight\", \"model.bert.encoder.layer.3.output.dense.bias\", \"model.bert.encoder.layer.3.output.LayerNorm.weight\", \"model.bert.encoder.layer.3.output.LayerNorm.bias\", \"model.bert.encoder.layer.4.attention.self.query.weight\", \"model.bert.encoder.layer.4.attention.self.query.bias\", \"model.bert.encoder.layer.4.attention.self.key.weight\", \"model.bert.encoder.layer.4.attention.self.key.bias\", \"model.bert.encoder.layer.4.attention.self.value.weight\", \"model.bert.encoder.layer.4.attention.self.value.bias\", \"model.bert.encoder.layer.4.attention.output.dense.weight\", \"model.bert.encoder.layer.4.attention.output.dense.bias\", \"model.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.4.intermediate.dense.weight\", \"model.bert.encoder.layer.4.intermediate.dense.bias\", \"model.bert.encoder.layer.4.output.dense.weight\", \"model.bert.encoder.layer.4.output.dense.bias\", \"model.bert.encoder.layer.4.output.LayerNorm.weight\", \"model.bert.encoder.layer.4.output.LayerNorm.bias\", \"model.bert.encoder.layer.5.attention.self.query.weight\", \"model.bert.encoder.layer.5.attention.self.query.bias\", \"model.bert.encoder.layer.5.attention.self.key.weight\", \"model.bert.encoder.layer.5.attention.self.key.bias\", \"model.bert.encoder.layer.5.attention.self.value.weight\", \"model.bert.encoder.layer.5.attention.self.value.bias\", \"model.bert.encoder.layer.5.attention.output.dense.weight\", \"model.bert.encoder.layer.5.attention.output.dense.bias\", \"model.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.5.intermediate.dense.weight\", \"model.bert.encoder.layer.5.intermediate.dense.bias\", \"model.bert.encoder.layer.5.output.dense.weight\", \"model.bert.encoder.layer.5.output.dense.bias\", \"model.bert.encoder.layer.5.output.LayerNorm.weight\", \"model.bert.encoder.layer.5.output.LayerNorm.bias\", \"model.bert.encoder.layer.6.attention.self.query.weight\", \"model.bert.encoder.layer.6.attention.self.query.bias\", \"model.bert.encoder.layer.6.attention.self.key.weight\", \"model.bert.encoder.layer.6.attention.self.key.bias\", \"model.bert.encoder.layer.6.attention.self.value.weight\", \"model.bert.encoder.layer.6.attention.self.value.bias\", \"model.bert.encoder.layer.6.attention.output.dense.weight\", \"model.bert.encoder.layer.6.attention.output.dense.bias\", \"model.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.6.intermediate.dense.weight\", \"model.bert.encoder.layer.6.intermediate.dense.bias\", \"model.bert.encoder.layer.6.output.dense.weight\", \"model.bert.encoder.layer.6.output.dense.bias\", \"model.bert.encoder.layer.6.output.LayerNorm.weight\", \"model.bert.encoder.layer.6.output.LayerNorm.bias\", \"model.bert.encoder.layer.7.attention.self.query.weight\", \"model.bert.encoder.layer.7.attention.self.query.bias\", \"model.bert.encoder.layer.7.attention.self.key.weight\", \"model.bert.encoder.layer.7.attention.self.key.bias\", \"model.bert.encoder.layer.7.attention.self.value.weight\", \"model.bert.encoder.layer.7.attention.self.value.bias\", \"model.bert.encoder.layer.7.attention.output.dense.weight\", \"model.bert.encoder.layer.7.attention.output.dense.bias\", \"model.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.7.intermediate.dense.weight\", \"model.bert.encoder.layer.7.intermediate.dense.bias\", \"model.bert.encoder.layer.7.output.dense.weight\", \"model.bert.encoder.layer.7.output.dense.bias\", \"model.bert.encoder.layer.7.output.LayerNorm.weight\", \"model.bert.encoder.layer.7.output.LayerNorm.bias\", \"model.bert.encoder.layer.8.attention.self.query.weight\", \"model.bert.encoder.layer.8.attention.self.query.bias\", \"model.bert.encoder.layer.8.attention.self.key.weight\", \"model.bert.encoder.layer.8.attention.self.key.bias\", \"model.bert.encoder.layer.8.attention.self.value.weight\", \"model.bert.encoder.layer.8.attention.self.value.bias\", \"model.bert.encoder.layer.8.attention.output.dense.weight\", \"model.bert.encoder.layer.8.attention.output.dense.bias\", \"model.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.8.intermediate.dense.weight\", \"model.bert.encoder.layer.8.intermediate.dense.bias\", \"model.bert.encoder.layer.8.output.dense.weight\", \"model.bert.encoder.layer.8.output.dense.bias\", \"model.bert.encoder.layer.8.output.LayerNorm.weight\", \"model.bert.encoder.layer.8.output.LayerNorm.bias\", \"model.bert.encoder.layer.9.attention.self.query.weight\", \"model.bert.encoder.layer.9.attention.self.query.bias\", \"model.bert.encoder.layer.9.attention.self.key.weight\", \"model.bert.encoder.layer.9.attention.self.key.bias\", \"model.bert.encoder.layer.9.attention.self.value.weight\", \"model.bert.encoder.layer.9.attention.self.value.bias\", \"model.bert.encoder.layer.9.attention.output.dense.weight\", \"model.bert.encoder.layer.9.attention.output.dense.bias\", \"model.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.9.intermediate.dense.weight\", \"model.bert.encoder.layer.9.intermediate.dense.bias\", \"model.bert.encoder.layer.9.output.dense.weight\", \"model.bert.encoder.layer.9.output.dense.bias\", \"model.bert.encoder.layer.9.output.LayerNorm.weight\", \"model.bert.encoder.layer.9.output.LayerNorm.bias\", \"model.bert.encoder.layer.10.attention.self.query.weight\", \"model.bert.encoder.layer.10.attention.self.query.bias\", \"model.bert.encoder.layer.10.attention.self.key.weight\", \"model.bert.encoder.layer.10.attention.self.key.bias\", \"model.bert.encoder.layer.10.attention.self.value.weight\", \"model.bert.encoder.layer.10.attention.self.value.bias\", \"model.bert.encoder.layer.10.attention.output.dense.weight\", \"model.bert.encoder.layer.10.attention.output.dense.bias\", \"model.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.10.intermediate.dense.weight\", \"model.bert.encoder.layer.10.intermediate.dense.bias\", \"model.bert.encoder.layer.10.output.dense.weight\", \"model.bert.encoder.layer.10.output.dense.bias\", \"model.bert.encoder.layer.10.output.LayerNorm.weight\", \"model.bert.encoder.layer.10.output.LayerNorm.bias\", \"model.bert.encoder.layer.11.attention.self.query.weight\", \"model.bert.encoder.layer.11.attention.self.query.bias\", \"model.bert.encoder.layer.11.attention.self.key.weight\", \"model.bert.encoder.layer.11.attention.self.key.bias\", \"model.bert.encoder.layer.11.attention.self.value.weight\", \"model.bert.encoder.layer.11.attention.self.value.bias\", \"model.bert.encoder.layer.11.attention.output.dense.weight\", \"model.bert.encoder.layer.11.attention.output.dense.bias\", \"model.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"model.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"model.bert.encoder.layer.11.intermediate.dense.weight\", \"model.bert.encoder.layer.11.intermediate.dense.bias\", \"model.bert.encoder.layer.11.output.dense.weight\", \"model.bert.encoder.layer.11.output.dense.bias\", \"model.bert.encoder.layer.11.output.LayerNorm.weight\", \"model.bert.encoder.layer.11.output.LayerNorm.bias\", \"model.bert.pooler.dense.weight\", \"model.bert.pooler.dense.bias\", \"model.classifier.weight\", \"model.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"model.mpnet.embeddings.position_ids\", \"model.mpnet.embeddings.word_embeddings.weight\", \"model.mpnet.embeddings.position_embeddings.weight\", \"model.mpnet.embeddings.LayerNorm.weight\", \"model.mpnet.embeddings.LayerNorm.bias\", \"model.mpnet.encoder.layer.0.attention.attn.q.weight\", \"model.mpnet.encoder.layer.0.attention.attn.q.bias\", \"model.mpnet.encoder.layer.0.attention.attn.k.weight\", \"model.mpnet.encoder.layer.0.attention.attn.k.bias\", \"model.mpnet.encoder.layer.0.attention.attn.v.weight\", \"model.mpnet.encoder.layer.0.attention.attn.v.bias\", \"model.mpnet.encoder.layer.0.attention.attn.o.weight\", \"model.mpnet.encoder.layer.0.attention.attn.o.bias\", \"model.mpnet.encoder.layer.0.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.0.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.0.intermediate.dense.weight\", \"model.mpnet.encoder.layer.0.intermediate.dense.bias\", \"model.mpnet.encoder.layer.0.output.dense.weight\", \"model.mpnet.encoder.layer.0.output.dense.bias\", \"model.mpnet.encoder.layer.0.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.0.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.1.attention.attn.q.weight\", \"model.mpnet.encoder.layer.1.attention.attn.q.bias\", \"model.mpnet.encoder.layer.1.attention.attn.k.weight\", \"model.mpnet.encoder.layer.1.attention.attn.k.bias\", \"model.mpnet.encoder.layer.1.attention.attn.v.weight\", \"model.mpnet.encoder.layer.1.attention.attn.v.bias\", \"model.mpnet.encoder.layer.1.attention.attn.o.weight\", \"model.mpnet.encoder.layer.1.attention.attn.o.bias\", \"model.mpnet.encoder.layer.1.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.1.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.1.intermediate.dense.weight\", \"model.mpnet.encoder.layer.1.intermediate.dense.bias\", \"model.mpnet.encoder.layer.1.output.dense.weight\", \"model.mpnet.encoder.layer.1.output.dense.bias\", \"model.mpnet.encoder.layer.1.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.1.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.2.attention.attn.q.weight\", \"model.mpnet.encoder.layer.2.attention.attn.q.bias\", \"model.mpnet.encoder.layer.2.attention.attn.k.weight\", \"model.mpnet.encoder.layer.2.attention.attn.k.bias\", \"model.mpnet.encoder.layer.2.attention.attn.v.weight\", \"model.mpnet.encoder.layer.2.attention.attn.v.bias\", \"model.mpnet.encoder.layer.2.attention.attn.o.weight\", \"model.mpnet.encoder.layer.2.attention.attn.o.bias\", \"model.mpnet.encoder.layer.2.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.2.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.2.intermediate.dense.weight\", \"model.mpnet.encoder.layer.2.intermediate.dense.bias\", \"model.mpnet.encoder.layer.2.output.dense.weight\", \"model.mpnet.encoder.layer.2.output.dense.bias\", \"model.mpnet.encoder.layer.2.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.2.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.3.attention.attn.q.weight\", \"model.mpnet.encoder.layer.3.attention.attn.q.bias\", \"model.mpnet.encoder.layer.3.attention.attn.k.weight\", \"model.mpnet.encoder.layer.3.attention.attn.k.bias\", \"model.mpnet.encoder.layer.3.attention.attn.v.weight\", \"model.mpnet.encoder.layer.3.attention.attn.v.bias\", \"model.mpnet.encoder.layer.3.attention.attn.o.weight\", \"model.mpnet.encoder.layer.3.attention.attn.o.bias\", \"model.mpnet.encoder.layer.3.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.3.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.3.intermediate.dense.weight\", \"model.mpnet.encoder.layer.3.intermediate.dense.bias\", \"model.mpnet.encoder.layer.3.output.dense.weight\", \"model.mpnet.encoder.layer.3.output.dense.bias\", \"model.mpnet.encoder.layer.3.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.3.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.4.attention.attn.q.weight\", \"model.mpnet.encoder.layer.4.attention.attn.q.bias\", \"model.mpnet.encoder.layer.4.attention.attn.k.weight\", \"model.mpnet.encoder.layer.4.attention.attn.k.bias\", \"model.mpnet.encoder.layer.4.attention.attn.v.weight\", \"model.mpnet.encoder.layer.4.attention.attn.v.bias\", \"model.mpnet.encoder.layer.4.attention.attn.o.weight\", \"model.mpnet.encoder.layer.4.attention.attn.o.bias\", \"model.mpnet.encoder.layer.4.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.4.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.4.intermediate.dense.weight\", \"model.mpnet.encoder.layer.4.intermediate.dense.bias\", \"model.mpnet.encoder.layer.4.output.dense.weight\", \"model.mpnet.encoder.layer.4.output.dense.bias\", \"model.mpnet.encoder.layer.4.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.4.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.5.attention.attn.q.weight\", \"model.mpnet.encoder.layer.5.attention.attn.q.bias\", \"model.mpnet.encoder.layer.5.attention.attn.k.weight\", \"model.mpnet.encoder.layer.5.attention.attn.k.bias\", \"model.mpnet.encoder.layer.5.attention.attn.v.weight\", \"model.mpnet.encoder.layer.5.attention.attn.v.bias\", \"model.mpnet.encoder.layer.5.attention.attn.o.weight\", \"model.mpnet.encoder.layer.5.attention.attn.o.bias\", \"model.mpnet.encoder.layer.5.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.5.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.5.intermediate.dense.weight\", \"model.mpnet.encoder.layer.5.intermediate.dense.bias\", \"model.mpnet.encoder.layer.5.output.dense.weight\", \"model.mpnet.encoder.layer.5.output.dense.bias\", \"model.mpnet.encoder.layer.5.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.5.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.6.attention.attn.q.weight\", \"model.mpnet.encoder.layer.6.attention.attn.q.bias\", \"model.mpnet.encoder.layer.6.attention.attn.k.weight\", \"model.mpnet.encoder.layer.6.attention.attn.k.bias\", \"model.mpnet.encoder.layer.6.attention.attn.v.weight\", \"model.mpnet.encoder.layer.6.attention.attn.v.bias\", \"model.mpnet.encoder.layer.6.attention.attn.o.weight\", \"model.mpnet.encoder.layer.6.attention.attn.o.bias\", \"model.mpnet.encoder.layer.6.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.6.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.6.intermediate.dense.weight\", \"model.mpnet.encoder.layer.6.intermediate.dense.bias\", \"model.mpnet.encoder.layer.6.output.dense.weight\", \"model.mpnet.encoder.layer.6.output.dense.bias\", \"model.mpnet.encoder.layer.6.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.6.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.7.attention.attn.q.weight\", \"model.mpnet.encoder.layer.7.attention.attn.q.bias\", \"model.mpnet.encoder.layer.7.attention.attn.k.weight\", \"model.mpnet.encoder.layer.7.attention.attn.k.bias\", \"model.mpnet.encoder.layer.7.attention.attn.v.weight\", \"model.mpnet.encoder.layer.7.attention.attn.v.bias\", \"model.mpnet.encoder.layer.7.attention.attn.o.weight\", \"model.mpnet.encoder.layer.7.attention.attn.o.bias\", \"model.mpnet.encoder.layer.7.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.7.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.7.intermediate.dense.weight\", \"model.mpnet.encoder.layer.7.intermediate.dense.bias\", \"model.mpnet.encoder.layer.7.output.dense.weight\", \"model.mpnet.encoder.layer.7.output.dense.bias\", \"model.mpnet.encoder.layer.7.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.7.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.8.attention.attn.q.weight\", \"model.mpnet.encoder.layer.8.attention.attn.q.bias\", \"model.mpnet.encoder.layer.8.attention.attn.k.weight\", \"model.mpnet.encoder.layer.8.attention.attn.k.bias\", \"model.mpnet.encoder.layer.8.attention.attn.v.weight\", \"model.mpnet.encoder.layer.8.attention.attn.v.bias\", \"model.mpnet.encoder.layer.8.attention.attn.o.weight\", \"model.mpnet.encoder.layer.8.attention.attn.o.bias\", \"model.mpnet.encoder.layer.8.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.8.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.8.intermediate.dense.weight\", \"model.mpnet.encoder.layer.8.intermediate.dense.bias\", \"model.mpnet.encoder.layer.8.output.dense.weight\", \"model.mpnet.encoder.layer.8.output.dense.bias\", \"model.mpnet.encoder.layer.8.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.8.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.9.attention.attn.q.weight\", \"model.mpnet.encoder.layer.9.attention.attn.q.bias\", \"model.mpnet.encoder.layer.9.attention.attn.k.weight\", \"model.mpnet.encoder.layer.9.attention.attn.k.bias\", \"model.mpnet.encoder.layer.9.attention.attn.v.weight\", \"model.mpnet.encoder.layer.9.attention.attn.v.bias\", \"model.mpnet.encoder.layer.9.attention.attn.o.weight\", \"model.mpnet.encoder.layer.9.attention.attn.o.bias\", \"model.mpnet.encoder.layer.9.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.9.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.9.intermediate.dense.weight\", \"model.mpnet.encoder.layer.9.intermediate.dense.bias\", \"model.mpnet.encoder.layer.9.output.dense.weight\", \"model.mpnet.encoder.layer.9.output.dense.bias\", \"model.mpnet.encoder.layer.9.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.9.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.10.attention.attn.q.weight\", \"model.mpnet.encoder.layer.10.attention.attn.q.bias\", \"model.mpnet.encoder.layer.10.attention.attn.k.weight\", \"model.mpnet.encoder.layer.10.attention.attn.k.bias\", \"model.mpnet.encoder.layer.10.attention.attn.v.weight\", \"model.mpnet.encoder.layer.10.attention.attn.v.bias\", \"model.mpnet.encoder.layer.10.attention.attn.o.weight\", \"model.mpnet.encoder.layer.10.attention.attn.o.bias\", \"model.mpnet.encoder.layer.10.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.10.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.10.intermediate.dense.weight\", \"model.mpnet.encoder.layer.10.intermediate.dense.bias\", \"model.mpnet.encoder.layer.10.output.dense.weight\", \"model.mpnet.encoder.layer.10.output.dense.bias\", \"model.mpnet.encoder.layer.10.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.10.output.LayerNorm.bias\", \"model.mpnet.encoder.layer.11.attention.attn.q.weight\", \"model.mpnet.encoder.layer.11.attention.attn.q.bias\", \"model.mpnet.encoder.layer.11.attention.attn.k.weight\", \"model.mpnet.encoder.layer.11.attention.attn.k.bias\", \"model.mpnet.encoder.layer.11.attention.attn.v.weight\", \"model.mpnet.encoder.layer.11.attention.attn.v.bias\", \"model.mpnet.encoder.layer.11.attention.attn.o.weight\", \"model.mpnet.encoder.layer.11.attention.attn.o.bias\", \"model.mpnet.encoder.layer.11.attention.LayerNorm.weight\", \"model.mpnet.encoder.layer.11.attention.LayerNorm.bias\", \"model.mpnet.encoder.layer.11.intermediate.dense.weight\", \"model.mpnet.encoder.layer.11.intermediate.dense.bias\", \"model.mpnet.encoder.layer.11.output.dense.weight\", \"model.mpnet.encoder.layer.11.output.dense.bias\", \"model.mpnet.encoder.layer.11.output.LayerNorm.weight\", \"model.mpnet.encoder.layer.11.output.LayerNorm.bias\", \"model.mpnet.encoder.relative_attention_bias.weight\", \"model.classifier.dense.weight\", \"model.classifier.dense.bias\", \"model.classifier.out_proj.weight\", \"model.classifier.out_proj.bias\". "
     ]
    }
   ],
   "source": [
    "# load the best val_loss model\n",
    "trained_model = SingleLabelClassifier.load_from_checkpoint(\n",
    "  trainer.checkpoint_callback.best_model_path,\n",
    "  num_classes=num_classes\n",
    ")\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = VapeSLCDataset(\n",
    "    val_df,\n",
    "    tokenizer,\n",
    "    max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "test_dataset = VapeSLCDataset(\n",
    "    test_df,\n",
    "    tokenizer,\n",
    "    max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(test_dataset):\n",
    "  _, prediction = trained_model(\n",
    "    item[\"input_ids\"].unsqueeze(dim=0).to(device), \n",
    "    item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "  )\n",
    "  predictions.append(prediction.flatten())\n",
    "  labels.append(item[\"labels\"].int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD=0.6\n",
    "accuracy(predictions, labels, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hf login\n",
    "# trained_model.model.push_to_hub('bert_pl_SLC', repo_url='AMITKESARI2000/bert_pl_SLC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get back single value labels from one-hot encoding\n",
    "y_pred = [np.argmax(pl, axis=0) for pl in predictions.numpy()]\n",
    "y_testl = [np.argmax(pl, axis=0) for pl in labels.numpy()]\n",
    "\n",
    "\n",
    "# y_pred = predictions.numpy()\n",
    "# y_testl = labels.numpy()\n",
    "# upper, lower = 1, 0\n",
    "# y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n",
    "print(classification_report(\n",
    "    y_testl,\n",
    "    y_pred,\n",
    "    target_names=name_classes,\n",
    "    zero_division=0,\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_testl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_testl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
